{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model for language translation\n",
    "\n",
    "In this notebook, I am implementing from scratch the Transformer, a network architecture that was proposed in the paper \"Attention Is All You Need\" (Vaswani et al., Attention is all you need, Advances in neural information processing systems, 2017) and I am applying it to translate sentences from Portuguese to English. The Transformer is based solely on attention mechanisms, without any use of LSTMs or RNNs.\n",
    "\n",
    "The dataset, the hyperparameters' values and the examples used for the evaluation were taken from https://www.tensorflow.org/tutorials/text/transformer for comparison reasons. The figures are taken from the above referenced paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Portuguese-English translation dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells which load and process the dataset were taken from https://www.tensorflow.org/tutorials/text/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = (\n",
    "    train_examples\n",
    "    .map(tf_encode) \n",
    "    .filter(filter_max_length)\n",
    "    # cache the dataset to memory to get a speedup while reading from it.\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE))\n",
    "\n",
    "val_preprocessed = (\n",
    "    val_examples\n",
    "    .map(tf_encode)\n",
    "    .filter(filter_max_length))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (train_preprocessed\n",
    "                 .padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "val_dataset = (val_preprocessed\n",
    "               .padded_batch(BATCH_SIZE,  padded_shapes=([None], [None])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8214\n",
      "8087\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_pt.vocab_size)\n",
    "print(tokenizer_en.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(tokenizer_en.vocab_size):\n",
    "#     print(i,tokenizer_en.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(tokenizer_pt.vocab_size):\n",
    "#     print(i,tokenizer_pt.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the training dataset in proper format for TensorFlow 1.15\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    X_train.append(inp.numpy())\n",
    "    Y_train.append(tar.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs():\n",
    "    \n",
    "    encoder_input = tf.placeholder(tf.int32,[None,None],name='encoder_input')\n",
    "    decoder_input = tf.placeholder(tf.int32,[None,None],name='decoder_input')\n",
    "    target = tf.placeholder(tf.int32,[None,None],name='target')\n",
    "    \n",
    "    return encoder_input, decoder_input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "<img src=\"images/positional_encoding.png\" style=\"width:100;height:100px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "        \n",
    "    pos = np.arange(0,position)\n",
    "    pos = pos.reshape(-1,1)\n",
    "    \n",
    "    div_term = 10000**(np.arange(0, d_model, 2)/d_model)\n",
    "        \n",
    "    pe = np.zeros((position, d_model))\n",
    "    pe[:,0::2] = np.sin(pos/div_term)\n",
    "    pe[:,1::2] = np.cos(pos/div_term)\n",
    "        \n",
    "    positional_enc = tf.convert_to_tensor(pe, dtype=tf.float32) # (position, d_model)\n",
    "    positional_enc = positional_enc[np.newaxis, :,:] # (1, position, d_model)\n",
    "        \n",
    "    return positional_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention\n",
    "\n",
    "<img src=\"images/scaled_dot_product_attention.png\" style=\"width:300;height:300px;\"> <br>\n",
    "<img src=\"images/attention.png\" style=\"width:100;height:100px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have an encoding for each word (token) of each sentence in the encoder or the decoder. In the calculations, we don't use the token directly, but the embedded representation of the word (token) projected to a subspace. \n",
    "\n",
    "The scaled dot product attention gives the encoding of each word (query) as the weighted sum of the similarity of the query word to all the other words (keys) in the sentence multiplied with the words of the sentence (values). \n",
    "\n",
    "For the Multi-Head Attention block in the encoder or the Masked Multi-Head Attention block in the Decoder, we are in the case of **self-attention**. In that case, we want to see how each word (query) of a sentence is related to all the other words (including itself) of the same sentence. \n",
    "\n",
    "The weights i.e. softmax(QK^T/sqrt(dk)),  are the similarity of the word (query) with each the words of the same sentence (keys). The weights are then multiplied with each word of the sentence (value) to get the encoding of the query. \n",
    "\n",
    "For the Multi-Head Attention block in the decoder, we are in the case of **attention**. In that case, we want to see how each word (query) of the decoder sentence is related to the words of the encoder sentence. The weights are the similarity of the word (query) of the decoder with each word of the encoder sentence (keys). The weights are then multiplied with each word of the encoder sentence (value) to get the encoding of the query (decoder word). \n",
    "\n",
    "If for some words (values) the mask is equal to 1 (i.e. the words have been padded or correspond to future words), we do not want to take into account these words in the encoding of the query. For this reason the weights, which will be multiplied with the words that have a mask of 1, are set equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    " \n",
    "    # queries, keys and values are the projected embeddings that have been split into a specific subspace\n",
    "    # The number of subspaces is equal to the number of attention heads\n",
    "    \n",
    "    # queries: (batch_size, Tq, dq)\n",
    "    # keys: (batch_size, Tk, dk)\n",
    "    # values: (batch_size, Tv, dv)\n",
    "    # where dq = dk = dv = d_model/num_heads and Tk = Tv (for self-attention Tq = Tk = Tv)\n",
    "    \n",
    "    \n",
    "    # Q*K^T/sqrt(dk)\n",
    "    product = tf.matmul(queries, tf.transpose(keys, [0, 2, 1]))  # (batch_size, Tq, Tk)\n",
    "    d_k = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    logits = product / tf.math.sqrt(d_k)\n",
    "\n",
    "    # mask has size either:\n",
    "    # 1. (batch_size, 1, Tv) used for encoder self-attention or encoder-decoder attention (2nd attention block of the \n",
    "    # decoder)\n",
    "    # or\n",
    "    # 2. (batch_size, Tq, Tv) used for decoder self-attention (1st attention block of the decoder)\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9) # making the locations where the token is 1, a very negative number so the weight \n",
    "                                # at this location becomes 0\n",
    "    \n",
    "    # softmax(Q*K^T/sqrt(dk))\n",
    "    # weights are the similarity of the query with each the words of keys\n",
    "    weights = tf.nn.softmax(logits, axis=-1)  # (batch_size, Tq, Tk)\n",
    "    # If the input sentence has N padding tokens at the end, then the matrix weights (think of it as shape (Tq,Tk))\n",
    "    # has the last N columns being 0.\n",
    "    # That means that at each column of matrix values (think of it as shape (Tv, dv)), the last N numbers \n",
    "    # of the column are not used since they are multiplied with 0. In other words the last N rows of matrix values \n",
    "    # are not used. The padding mask disables the last N rows of matrix values.\n",
    "    \n",
    "    # softmax(Q*K^T/sqrt(dk))*V\n",
    "    output = tf.matmul(weights, values)  # (batch_size, Tq, dv)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want to find the encoding of the first word of the following sentence which is an input of the encoder.\n",
    "\n",
    "If the input sentence consists of the following 40 tokens\n",
    "\n",
    "[[8214 149    6 1748   50    5   34    3 5033  887  238 8215    0    0 0    0    0    0    0    0    0    0    0    0    0    0    0    0 0    0    0    0    0    0    0    0    0    0    0    0]]\n",
    "     \n",
    "then the encoder padding mask is the a tensor of shape (1,1,40) and has values equal to 1 for the N padding tokens\n",
    "\n",
    "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
    "\n",
    "Since the input sentence has N padding tokens at the end, then the matrix weights has the last N columns being 0. The next tensor shows the weights of the query with each of the keys (relation between the first word with all other words in the sentence). Since the last N tokens are the padding token, we do not take them into account by setting equal to 0 the corresponding weights that they will be multiplied with the values. \n",
    "   \n",
    "The weights for the 1st query is a tensor of shape (1,1,40) with the last N weights being 0\n",
    "\n",
    "[[[0.04349909 0.06763001 0.09824815 0.04452311 0.06439371  0.10503896 \n",
    "   0.04398107 0.06941279 0.09019028 0.07023811 0.07473257 0.22811212\n",
    "   0         0         0         0         0         0\n",
    "   0         0         0         0         0         0\n",
    "   0         0         0         0         0         0\n",
    "   0         0         0         0         0         0\n",
    "   0         0         0 0]]]\n",
    "   \n",
    "   \n",
    "and are multiplied with the values which has shape (1,40,d_model/num_heads) to get the encoding of the query (1st word) which has size (1,1,d_model/num_heads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "<img src=\"images/mha.png\" style=\"width:300;height:300px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(values, keys, queries, mask, d_model, num_heads, scope):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "        # queries, keys and values are the embeddings of the input words (they haven't been projected yet, just embeddings)\n",
    "        # queries: (batch_size, Tq, d_model)\n",
    "        # keys: (batch_size, Tk, d_model)\n",
    "        # values: (batch_size, Tv, d_model)\n",
    "        \n",
    "        # The input embeddings (i.e. queries, keys and values) are projected to a representation space \n",
    "        # by applying a huge linear transformation\n",
    "        queries_lin = tf.contrib.layers.fully_connected(queries, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform()) \n",
    "        # (batch_size, Tq, d_model)\n",
    "        keys_lin = tf.contrib.layers.fully_connected(keys, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())\n",
    "        # (batch_size, Tk, d_model)\n",
    "        values_lin = tf.contrib.layers.fully_connected(values, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())\n",
    "        # (batch_size, Tv, d_model)\n",
    "            \n",
    "        # We split the projected embeddings into subspaces.\n",
    "        # The number of subspaces is equal to the number of attention heads\n",
    "        queries_split = tf.split(queries_lin, num_heads, axis=2) \n",
    "        keys_split = tf.split(keys_lin, num_heads, axis=2) \n",
    "        values_split = tf.split(values_lin, num_heads, axis=2) \n",
    "        # lists of num_heads elements with each of them having size (batch_size, T, d_model/num_heads) \n",
    "        # where T is Tq or Tk or Tv\n",
    "        \n",
    "        attention = []\n",
    "        # We get the encoding of each word (query) at each representation subspace. It is saved in attention_i.\n",
    "        for head_i in range(num_heads):\n",
    "            attention_i = scaled_dot_product_attention(queries_split[head_i], keys_split[head_i], \n",
    "                                                       values_split[head_i], mask)\n",
    "            # (batch_size, Tq, d_model/num_heads)\n",
    "            attention.append(attention_i)\n",
    "        \n",
    "        concat_attention = tf.concat(attention, axis = 2) # (batch_size, Tq, d_model)\n",
    "        \n",
    "        output = tf.contrib.layers.fully_connected(concat_attention, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())\n",
    "        # (batch_size, Tq, d_model)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position wise feed forward network\n",
    "\n",
    "<img src=\"images/pwffn.png\" style=\"width:50;height:50px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(input_, d_model, d_ff, scope):\n",
    "\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        x1 = tf.contrib.layers.fully_connected(input_, d_ff, activation_fn = tf.nn.relu,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())  \n",
    "        # (batch_size, T, d_ff)\n",
    "        ffn = tf.contrib.layers.fully_connected(x1, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform()) \n",
    "        # (batch_size, T, d_model)\n",
    "            \n",
    "    return ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "<img src=\"images/encoder_layer.png\" style=\"width:400;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(x, mask, d_model, d_ff, num_heads, rate, scope):\n",
    "\n",
    "    # x is the embedding of the input tokens and has size (batch_size, T_x, d_model)\n",
    "    # mask is the mask used in the attention block of the encoder (encoder self-attention)\n",
    "    # to mask the padding tokens of the encoder input and has size (batch_size, 1, T_x)\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        # encoder self-attention\n",
    "        attention_output = multi_head_attention(x, x, x, mask, d_model, num_heads, 'mha') \n",
    "        attention_output = tf.nn.dropout(attention_output, rate = rate)\n",
    "        # residual connection\n",
    "        output_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output) \n",
    "    \n",
    "        ffn_output = point_wise_feed_forward_network(output_1, d_model, d_ff, 'pwffn')\n",
    "        ffn_output = tf.nn.dropout(ffn_output, rate = rate)\n",
    "        # residual connection\n",
    "        output_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(output_1 + ffn_output) \n",
    "    \n",
    "    return output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, mask, num_layers, d_model, num_heads, d_ff, input_vocab_size, maximum_position_input, rate, scope):\n",
    "\n",
    "    # x is the embedding of the input tokens and has size (batch_size, T_x, d_model)\n",
    "    # mask is the mask used in the attention block of the encoder (encoder self-attention)\n",
    "    # to mask the padding tokens of the encoder input and has size (batch_size, 1, T_x)\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        T_x = tf.shape(x)[1]\n",
    "        pos_encoding = positional_encoding(maximum_position_input, d_model) # (1, maximum_position_input, d_model)\n",
    "        # maximum_position_input is equal to input_vocab_size\n",
    "        \n",
    "        # get the embeddings of the input tokens x\n",
    "        x = tf.keras.layers.Embedding(input_vocab_size, d_model)(x) # (batch_size, T_x, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "        # add the positional encoding\n",
    "        x += pos_encoding[:, :T_x, :] # pos_encoding[:, :T_x, :] has shape (1, T_x, d_model)\n",
    "        # x has shape (batch_size, T_x, d_model)\n",
    "        x = tf.nn.dropout(x, rate = rate)\n",
    "    \n",
    "        for i in range(num_layers):\n",
    "            x = encoder_layer(x, mask, d_model, d_ff, num_heads, rate, 'Encoder_layer_'+str(i+1))\n",
    "    \n",
    "    return x  # (batch_size, T_x, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "\n",
    "<img src=\"images/decoder_layer.png\" style=\"width:500;height:500px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(y, encoder_output, mask_1, mask_2, d_model, d_ff, num_heads, rate, scope):\n",
    "\n",
    "    # y is the embedding of the input tokens and has size (batch_size, T_y, d_model)\n",
    "    \n",
    "    # mask_1 is the mask used in the 1st attention block of the decoder (decoder self-attention). \n",
    "    # It is a combination of the mask of the padding tokens of the decoder input \n",
    "    # and of the mask that prevents positions from attending to subsequent positions of the decoder input. \n",
    "    # It has size (batch_size, T_y, T_y)\n",
    "    \n",
    "    # mask_2 is used in the 2nd attention block of the decoder (encoder-decoder attention) \n",
    "    # to mask the padding tokens of the encoder outputs (input in the 2nd attention block of the decoder) \n",
    "    # and has size (batch_size, 1, T_x)\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        # decoder self-attention\n",
    "        attention_output_1 = multi_head_attention(y, y, y, mask_1, \n",
    "                                                                    d_model, num_heads, 'mha_1') \n",
    "        attention_output_1 = tf.nn.dropout(attention_output_1, rate = rate)\n",
    "        # residual connection\n",
    "        output_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(y + attention_output_1)\n",
    "\n",
    "        # encoder-decoder attention\n",
    "        attention_output_2 = multi_head_attention(encoder_output, encoder_output, output_1, \n",
    "                                                                    mask_2, d_model, num_heads, 'mha_2')\n",
    "        attention_output_2 = tf.nn.dropout(attention_output_2, rate = rate)\n",
    "        # residual connection\n",
    "        output_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output_2 + output_1) \n",
    "    \n",
    "        ffn_output = point_wise_feed_forward_network(output_2, d_model, d_ff, 'pwffn')\n",
    "        ffn_output = tf.nn.dropout(ffn_output, rate = rate)\n",
    "        # residual connection\n",
    "        output_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(output_2 + ffn_output)\n",
    "    \n",
    "    return output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(y, encoder_output, mask_1, mask_2, num_layers, d_model, num_heads, d_ff, \n",
    "                        target_vocab_size, maximum_position_target, rate, scope):\n",
    "    \n",
    "    # y is the sequence of words (output tokens) (batch_size, T_y)\n",
    "    \n",
    "    # mask_1 is the mask used in the 1st attention block of the decoder (decoder self-attention). \n",
    "    # It is a combination of the mask of the padding tokens of the decoder input \n",
    "    # and of the mask that prevents positions from attending to subsequent positions of the decoder input. \n",
    "    # It has size (batch_size, T_y, T_y)\n",
    "    \n",
    "    # mask_2 is used in the 2nd attention block of the decoder (encoder-decoder attention) \n",
    "    # to mask the padding tokens of the encoder outputs (input in the 2nd attention block of the decoder) \n",
    "    # and has size (batch_size, 1, T_x)\n",
    "\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):   \n",
    "        \n",
    "        T_y = tf.shape(y)[1] \n",
    "        attention_weights = {}    \n",
    "        pos_encoding = positional_encoding(maximum_position_target, d_model) # (1, maximum_position_target, d_model)\n",
    "        # maximum_position_target is equal to target_vocab_size\n",
    "    \n",
    "        # get the embeddings of the input tokens y\n",
    "        y = tf.keras.layers.Embedding(target_vocab_size, d_model)(y)  # (batch_size, T_y, d_model)\n",
    "        y *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "        y += pos_encoding[:, :T_y, :] # pos_encoding[:, :T_y, :] has shape (1, T_y, d_model)\n",
    "        # y has shape (batch_size, T_y, d_model)\n",
    "        y = tf.nn.dropout(y, rate = rate)\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            y = decoder_layer(y, encoder_output, mask_1, mask_2, d_model, \n",
    "                                              d_ff, num_heads, rate, 'Decoder_layer_'+str(i+1))\n",
    "    \n",
    "    return y # (batch_size, T_y, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask_function(input_):\n",
    "    \n",
    "    # It masks all the padding tokens of the sequence and thus the model does not treat padding as input.\n",
    "    # The tokens which are masked to 1 will not be used.\n",
    "    \n",
    "    # input_ has size # (batch_size, T)\n",
    "    padding_mask = tf.cast(tf.math.equal(input_, 0), tf.float32) # (batch_size, T)\n",
    "  \n",
    "    # add extra dimension to add the padding to the scaled dot product attention logits\n",
    "    # which have size (batch_size, Tq, Tk)\n",
    "    return padding_mask[:, tf.newaxis, :]  # (batch_size, 1, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input sentence consists of the following 40 tokens\n",
    "\n",
    "[[8214 149    6 1748   50    5   34    3 5033  887  238 8215    0    0 0    0    0    0    0    0    0    0    0    0    0    0    0    0 0    0    0    0    0    0    0    0    0    0    0    0]]\n",
    "     \n",
    "then the padding mask is the a tensor of shape (1,1,40) and has values equal to 1 for the N padding tokens,\n",
    "so that the weights for these locations will become 0 and these words will not be used\n",
    "\n",
    "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_word_mask_function(batch_size, seq_len):\n",
    "    \n",
    "    # The future word mask masks the future tokens of the sequence, preventing positions from attending \n",
    "    # to subsequent positions of the decoder input. \n",
    "    # The tokens which are masked to 1 will not be used.\n",
    "    # This means that in order to predict the second word, only the first word will be used. To predict the third word,\n",
    "    # only the first and second will be used.\n",
    "        \n",
    "    # seq_len is the length of the target sequence of words and is equal to T_y\n",
    "    diag_vals = tf.ones((seq_len, seq_len))  # (T_y, T_y)\n",
    "    tril = 1 - tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_y, T_y)\n",
    "    # Return a dense (batch) matrix representing this operator \n",
    "    future_word_mask = tf.tile(tril[tf.newaxis, :, :], [batch_size, 1, 1]) # (batch_size, T_y, T_y)\n",
    "\n",
    "    return future_word_mask  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For batch_size = 2 and T_y = 5 \n",
    "\n",
    "diag_vals is a tensor of shape (5,5)\n",
    "\n",
    "[[1 1 1 1 1]\n",
    "\n",
    " [1 1 1 1 1]\n",
    " \n",
    " [1 1 1 1 1]\n",
    " \n",
    " [1 1 1 1 1]\n",
    " \n",
    " [1 1 1 1 1]]\n",
    " \n",
    "tril is a tensor of shape (5,5) with values of 1 for the locations of the future words,\n",
    "so that the weights for the future locations will become 0\n",
    "\n",
    "[[0 1 1 1 1]\n",
    "\n",
    " [0 0 1 1 1]\n",
    " \n",
    " [0 0 0 1 1]\n",
    " \n",
    " [0 0 0 0 1]\n",
    " \n",
    " [0 0 0 0 0]]\n",
    " \n",
    " Thus, the future_word_mask is a tensor of size (2,5,5)\n",
    " \n",
    " [[[0 1 1 1 1]\n",
    " \n",
    "  [0 0 1 1 1]\n",
    " \n",
    " [0 0 0 1 1]\n",
    " \n",
    " [0 0 0 0 1]\n",
    " \n",
    " [0 0 0 0 0]]\n",
    "\n",
    " [[0 1 1 1 1]\n",
    " \n",
    " [0 0 1 1 1]\n",
    " \n",
    " [0 0 0 1 1]\n",
    " \n",
    " [0 0 0 0 1]\n",
    " \n",
    " [0 0 0 0 0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(encoder_input, decoder_input):\n",
    "    \n",
    "    # encoder_input (batch_size, T_x)\n",
    "    # decoder_input (batch_size, T_y)\n",
    "    \n",
    "    # encoder_mask is used in the attention block of the encoder (encoder self-attention)\n",
    "    # to mask the padding tokens of the encoder input\n",
    "    encoder_mask = padding_mask_function(encoder_input) # (batch_size, 1, T_x)\n",
    "    \n",
    "    # decoder_mask_1 is used in the 1st attention block of the decoder (decoder self-attention) \n",
    "    # to mask the padding tokens of the decoder input \n",
    "    # and prevent positions from attending to subsequent positions of the decoder input \n",
    "    \n",
    "    # masks the future tokens of the sequence\n",
    "    future_word_mask = future_word_mask_function(tf.shape(decoder_input)[0], tf.shape(decoder_input)[1]) # (batch_size, T_y, T_y)\n",
    "    # masks the padding tokens of the decoder input\n",
    "    decoder_input_padding_mask = padding_mask_function(decoder_input) # (batch_size, 1, T_y)\n",
    "    # combination of the future_word_mask and the decoder_input_padding_mask masks\n",
    "    decoder_mask_1 = tf.maximum(decoder_input_padding_mask, future_word_mask) # (batch_size, T_y, T_y)\n",
    "  \n",
    "    # decoder_mask_2 is used in the 2nd attention block of the decoder (encoder-decoder attention)\n",
    "    # to mask the padding tokens of the encoder outputs\n",
    "    decoder_mask_2 = padding_mask_function(encoder_input) # (batch_size, 1, T_x)\n",
    "  \n",
    "    return encoder_mask, decoder_mask_1, decoder_mask_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the decoder_mask_1 \n",
    "\n",
    "If the target sentence consists of the following 39 tokens\n",
    "\n",
    "[[8087    4  258   10   51    8   41   21  118    2 8088    0    0    0 0    0    0    0    0    0    0    0    0    0    0    0    0    0 0    0    0    0    0    0    0    0    0    0    0]]\n",
    "\n",
    "The decoder_input_padding_mask is a tensor of shape (1,1,39)\n",
    "\n",
    "[[[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
    "   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]]\n",
    "   \n",
    "The future_word_mask is a tensor of size (1,39,39)\n",
    "\n",
    "[[[0 1 1 ... 1 1 1]\n",
    "\n",
    "  [0 0 1 ... 1 1 1]\n",
    "  \n",
    "  [0 0 0 ... 1 1 1]\n",
    "  \n",
    "  ...\n",
    "  \n",
    "  [0 0 0 ... 0 1 1]\n",
    "  \n",
    "  [0 0 0 ... 0 0 1]\n",
    "  \n",
    "  [0 0 0 ... 0 0 0]]]\n",
    "  \n",
    "The 3rd row of the future_word_mask is \n",
    " \n",
    " [[[0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
    "   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]]\n",
    "   \n",
    "and thus the 3rd row of the  decoder_mask_1 is\n",
    "\n",
    " [[[0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
    "   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]]\n",
    "\n",
    "The 30th row of the future_word_mask is \n",
    "\n",
    "[[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    "   0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]]]\n",
    "   \n",
    "and thus the 30th row of the  decoder_mask_1 is  \n",
    "\n",
    "[[[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
    "   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on decoder_mask_2\n",
    "\n",
    "decoder_mask_2 = padding_mask_function(encoder_input) takes as input the encoder input. The encoder input has padding tokens.\n",
    "\n",
    "At the 2nd attention block of the decoder we are in the case of encoder-decoder attention. In that case, we want to see how each word (query) of the decoder sentence is related to the words of the encoder sentence. The weights are the similarity of the word (query) of the decoder with each word of the encoder sentence (keys). The weights are then multiplied with each word of the encoder sentence (value) to get the encoding of the query (decoder word). \n",
    "\n",
    "The keys and values that are used as inputs to the Multi-Head Attention block in the decoder\n",
    "are the outputs of the encoder. The encoder output has padding at the locations that the encoder input has them. Thus, in the encoding of the decoder query, we do not want to use these values (encoder output) which correspond to padding locations. For this reason the weights that are multiplied with the values (encoder output) that correspond to padding should be set to 0 and thus the mask at these locations should be 1. \n",
    "\n",
    "Since the encoder output has padding at the locations that the encoder input has padding, we use as argument to the padding_mask_function the encoder_input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(encoder_input, decoder_input, encoder_mask, decoder_mask_1, decoder_mask_2, num_layers, d_model, num_heads, d_ff,\n",
    "                   input_vocab_size, target_vocab_size, maximum_position_input, maximum_position_target, rate):\n",
    "    \n",
    "    \n",
    "    encoder_output = encoder(encoder_input, encoder_mask, num_layers, d_model, num_heads, d_ff, input_vocab_size, \n",
    "                                        maximum_position_input, rate, 'Encoder') # (batch_size, T_x, d_model)\n",
    "\n",
    "    decoder_output = decoder(decoder_input, encoder_output, decoder_mask_1, decoder_mask_2, \n",
    "                                        num_layers, d_model, num_heads, d_ff, target_vocab_size, \n",
    "                                        maximum_position_target, rate, 'Decoder') # (batch_size, T_y, d_model)\n",
    "\n",
    "    final_output = tf.contrib.layers.fully_connected(decoder_output, target_vocab_size, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform()) \n",
    "    # (batch_size, T_y, target_vocab_size)\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "\n",
    "<img src=\"images/lrate.png\" style=\"width:50;height:50px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_function(d_model, step_num):\n",
    "    \n",
    "    warmup_steps = 4000\n",
    "\n",
    "    step_num = tf.cast(step_num + 1,tf.float32)\n",
    "    d_model = tf.cast(d_model, tf.float32)\n",
    "    \n",
    "    part1 = tf.math.rsqrt(step_num)\n",
    "    part2 = step_num * (warmup_steps ** (-1.5))\n",
    "    \n",
    "    learning_rate = tf.math.rsqrt(d_model) * tf.math.minimum(part1, part2)\n",
    "    \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target, prediction): # loss per batch\n",
    "    # target: (batch_size, T_y)\n",
    "    # prediction: (batch_size, T_y, tokenizer_en.vocab_size)\n",
    "    \n",
    "    # we find the locations where the target sequence does not have padding tokens\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0)) #(batch_size, T_y)\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(target, prediction)\n",
    "    # (batch_size, T_y)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype) # mask values are of same type with loss values\n",
    "    loss *= mask # we do not want to include in the loss the padding target tokens,\n",
    "    # so we only use the predictions for the real sentence and not for the 0s we add at the end\n",
    "    \n",
    "    # to find the mean value of the loss per batch we divide the loss \n",
    "    # with the number of tokens at this batch that are not padded (i.e. not 0)\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the target sentence consists of the following 39 tokens\n",
    "\n",
    "[[8087    4  258   10   51    8   41   21  118    2 8088    0    0    0 0    0    0    0    0    0    0    0    0    0    0    0    0    0 0    0    0    0    0    0    0    0    0    0    0]]\n",
    "\n",
    "then the mask is a Boolean tensor \n",
    "\n",
    "[[ True  True  True  True  True  True  True  True  True  True  True False\n",
    "  False False False False False False False False False False False False\n",
    "  False False False False False False False False False False False False\n",
    "  False False False]]\n",
    "  \n",
    "which will be converted so that its sum is equal to 11 which is the number of  non-padding tokens\n",
    "\n",
    "### Comment on multiplying loss with the mask\n",
    "\n",
    "The loss computes the difference between the target English translation and the predicted English translation. We do not want to include in the loss the padding target tokens, so we only use the predictions for the real sentence and not for the 0s we add at the end. For this reason we multiply the loss with a mask, that is 0 at the locations that the target English translation has the padding tokens. \n",
    "\n",
    "The multiplication of the loss with the mask is done for the 2nd attention block of the decoder (encoder-decoder attention). The decoder_mask_2 masks only the padding tokens from the encoder output, which is input (treated as keys and values) to 2nd attention block of the decoder, and not the padding tokens for the decoder part which is the query. \n",
    "\n",
    "But the padding tokens of the queries are also used in the scaled dot product attention, i.e. we find an encoding for them since they are treated as a query too, and they should not. Thus, we need to multiply the loss with this new mask, so the encoding of the decoder padding tokens is not taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(target, prediction):\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(target,tf.cast(tf.argmax(prediction, axis=-1),dtype=tf.int32)),dtype=tf.float32))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_function(loss, d_model):\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False) \n",
    "    learning_rate = learning_rate_function(d_model, global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09).minimize(loss, global_step)\n",
    "     \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_model:\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, target_vocab_size, maximum_position_input, \n",
    "                       maximum_position_target, rate):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.encoder_input, self.decoder_input, self.target = build_inputs()\n",
    "        \n",
    "        # masks\n",
    "        encoder_mask, decoder_mask_1, decoder_mask_2 = create_masks(self.encoder_input, self.decoder_input)\n",
    "        \n",
    "        self.predictions = transformer(self.encoder_input, self.decoder_input, encoder_mask, decoder_mask_1, \n",
    "                                     decoder_mask_2,  num_layers, d_model, num_heads, d_ff, input_vocab_size, \n",
    "                                     target_vocab_size, maximum_position_input, maximum_position_target, rate)\n",
    "        \n",
    "        # Loss\n",
    "        self.loss = loss_function(self.target, self.predictions)\n",
    "        \n",
    "        # Accuracy\n",
    "        self.accuracy = accuracy_function(self.target, self.predictions)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optimizer_function(self.loss, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "d_ff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2 # 2 is added for the <SOS> (8214) and <EOS> (8215) tokens\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2 # 2 is added for the <SOS> (8087) and <EOS> (8088) tokens\n",
    "dropout_rate = 0.1\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 Mean loss  : 7.1029 Mean accuracy  : 0.0522\n",
      "Epoch: 2/20 Mean loss  : 4.9797 Mean accuracy  : 0.1266\n",
      "Epoch: 3/20 Mean loss  : 4.4320 Mean accuracy  : 0.1524\n",
      "Epoch: 4/20 Mean loss  : 3.9348 Mean accuracy  : 0.1803\n",
      "Epoch: 5/20 Mean loss  : 3.4771 Mean accuracy  : 0.2061\n",
      "Epoch: 6/20 Mean loss  : 3.0921 Mean accuracy  : 0.2256\n",
      "Epoch: 7/20 Mean loss  : 2.7134 Mean accuracy  : 0.2461\n",
      "Epoch: 8/20 Mean loss  : 2.3953 Mean accuracy  : 0.2646\n",
      "Epoch: 9/20 Mean loss  : 2.1658 Mean accuracy  : 0.2783\n",
      "Epoch: 10/20 Mean loss  : 1.9901 Mean accuracy  : 0.2887\n",
      "Epoch: 11/20 Mean loss  : 1.8485 Mean accuracy  : 0.2972\n",
      "Epoch: 12/20 Mean loss  : 1.7334 Mean accuracy  : 0.3048\n",
      "Epoch: 13/20 Mean loss  : 1.6355 Mean accuracy  : 0.3109\n",
      "Epoch: 14/20 Mean loss  : 1.5468 Mean accuracy  : 0.3173\n",
      "Epoch: 15/20 Mean loss  : 1.4755 Mean accuracy  : 0.3218\n",
      "Epoch: 16/20 Mean loss  : 1.4069 Mean accuracy  : 0.3267\n",
      "Epoch: 17/20 Mean loss  : 1.3514 Mean accuracy  : 0.3307\n",
      "Epoch: 18/20 Mean loss  : 1.2965 Mean accuracy  : 0.3346\n",
      "Epoch: 19/20 Mean loss  : 1.2485 Mean accuracy  : 0.3381\n",
      "Epoch: 20/20 Mean loss  : 1.2033 Mean accuracy  : 0.3417\n",
      "Time taken for ALL epochs: 1291.9831004142761 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.disable_eager_execution()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = transformer_model(num_layers = num_layers, d_model = d_model, num_heads = num_heads, d_ff = d_ff, \n",
    "                          input_vocab_size = input_vocab_size, target_vocab_size = target_vocab_size, \n",
    "                          maximum_position_input = input_vocab_size, maximum_position_target = target_vocab_size, rate = dropout_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=num_epochs) \n",
    "checkpoint_folder = 'checkpoints_transformer_language_translation'\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    train_loss_per_step = []\n",
    "    train_loss_per_epoch = []\n",
    "    train_accuracy_per_epoch = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss_per_batch = []\n",
    "        train_accuracy_per_batch = []\n",
    "        \n",
    "        for batch in range(len(X_train)):\n",
    "            \n",
    "            encoder_input = X_train[batch]\n",
    "            \n",
    "            # if the target sentence is: \"<SOS> this is a problem we have to solve <EOS>\"\n",
    "            # then for each token that is given as input to the decoder, the target should be the next token\n",
    "            # For instance, to predict \"this\" the input token to the decoder should be \"<SOS>\" and \n",
    "            # the target should be \"this\" \n",
    "            decoder_input = Y_train[batch][:, :-1] # \"<SOS> this is a problem we have to solve\" \n",
    "            target = Y_train[batch][:, 1:]   # \"this is a problem we have to solve <EOS>\"\n",
    "\n",
    "            feed = {model.encoder_input: encoder_input,\n",
    "                   model.decoder_input: decoder_input,\n",
    "                   model.target: target}\n",
    "\n",
    "    \n",
    "            batch_predictions, batch_loss, batch_accuracy, _ = sess.run([model.predictions,\n",
    "                                                 model.loss,\n",
    "                                                 model.accuracy,\n",
    "                                                 model.optimizer],\n",
    "                                                 feed_dict=feed)\n",
    "        \n",
    "            \n",
    "            train_loss_per_batch.append(batch_loss)\n",
    "            train_loss_per_step.append(batch_loss)\n",
    "            train_accuracy_per_batch.append(batch_accuracy)\n",
    "            \n",
    "        train_loss_per_epoch.append(np.mean(train_loss_per_batch))\n",
    "        train_accuracy_per_epoch.append(np.mean(train_accuracy_per_batch))\n",
    "        \n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, num_epochs),\n",
    "              \"Mean loss  : {:.4f}\".format(np.mean(train_loss_per_batch)),\n",
    "              \"Mean accuracy  : {:.4f}\".format(np.mean(train_accuracy_per_batch)))\n",
    "        \n",
    "    saver.save(sess, checkpoint_folder+'/transformer.ckpt')\n",
    "    print ('Time taken for ALL epochs: {} secs\\n'.format(time.time() - start_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures of loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1,num_epochs+1), train_loss_per_epoch)\n",
    "plt.ylabel('mean loss per epoch')\n",
    "plt.xlabel('epoch')\n",
    "fig.savefig('./'+ checkpoint_folder +'/mean loss per epoch.png')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1,num_epochs+1), train_accuracy_per_epoch)\n",
    "plt.ylabel('mean accuracy per epoch')\n",
    "plt.xlabel('epoch')\n",
    "fig.savefig('./'+ checkpoint_folder +'/mean accuracy per epoch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The funtions 'evaluate' and 'translate' and the testing examples were taken from https://www.tensorflow.org/tutorials/text/transformer which uses TensorFlow 2.0 and were modified accordingly for TensorFlow 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "\n",
    "    dropout_rate = 0\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "    \n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = np.expand_dims(inp_sentence, 0)\n",
    "    \n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = np.expand_dims(decoder_input, 0)\n",
    "\n",
    "    model = transformer_model(num_layers = num_layers, d_model = d_model, num_heads = num_heads, d_ff = d_ff, \n",
    "                          input_vocab_size = input_vocab_size, target_vocab_size = target_vocab_size, \n",
    "                          maximum_position_input = input_vocab_size, maximum_position_target = target_vocab_size, rate = dropout_rate)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpoint_folder))\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            \n",
    "            feed = {model.encoder_input: encoder_input,\n",
    "                   model.decoder_input: output,\n",
    "                   model.target: output}\n",
    "            \n",
    "            batch_predictions = sess.run(model.predictions, feed_dict=feed)\n",
    "                    \n",
    "            # select the last word from the Ty dimension\n",
    "            predictions = batch_predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "            \n",
    "            predicted_id = np.argmax(predictions, axis=-1)\n",
    "            predicted_id = predicted_id.astype(int)\n",
    "            \n",
    "            # return the result if the predicted_id is equal to the end token\n",
    "            if predicted_id == tokenizer_en.vocab_size+1:\n",
    "                return np.squeeze(output, axis=0)\n",
    "        \n",
    "            # concatentate the predicted_id to the output which is given to the decoder # as its input.\n",
    "            output = np.concatenate((output, predicted_id), axis=-1)\n",
    "    \n",
    "        return np.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \n",
    "    result = evaluate(sentence)\n",
    "  \n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer_language_translation/transformer.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer_language_translation/transformer.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este é um problema que temos que resolver.\n",
      "Predicted translation: this is a problem we have to solve .\n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer_language_translation/transformer.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer_language_translation/transformer.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: os meus vizinhos ouviram sobre esta ideia.\n",
      "Predicted translation: my neighbors heard about this idea .\n",
      "Real translation: and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer_language_translation/transformer.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer_language_translation/transformer.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n",
      "Predicted translation: so i 'm going to share with you a few stories of some magic things that happened .\n",
      "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n"
     ]
    }
   ],
   "source": [
    "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Encoder/embedding/embeddings:0' shape=(8216, 128) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/layer_normalization/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/layer_normalization/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/layer_normalization_1/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/layer_normalization_1/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/layer_normalization_2/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/layer_normalization_2/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/layer_normalization_3/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/layer_normalization_3/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/layer_normalization_4/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/layer_normalization_4/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/layer_normalization_5/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/layer_normalization_5/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/layer_normalization_6/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/layer_normalization_6/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/layer_normalization_7/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/layer_normalization_7/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/embedding_1/embeddings:0' shape=(8089, 128) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_8/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_8/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_9/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_9/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_10/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_10/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_11/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_11/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_12/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_12/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_13/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_13/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_14/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_14/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_15/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_15/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_16/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_16/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_17/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_17/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_18/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_18/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_19/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_19/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'fully_connected/weights:0' shape=(128, 8089) dtype=float32_ref>\n",
      "<tf.Variable 'fully_connected/biases:0' shape=(8089,) dtype=float32_ref>\n",
      "total number of trainable parameters:  4981913\n"
     ]
    }
   ],
   "source": [
    "model = transformer_model(num_layers = num_layers, d_model = d_model, num_heads = num_heads, d_ff = d_ff, \n",
    "                          input_vocab_size = input_vocab_size, target_vocab_size = target_vocab_size, \n",
    "                          maximum_position_input = input_vocab_size, maximum_position_target = target_vocab_size, rate = dropout_rate)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        print(variable)\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "\n",
    "        total_parameters += variable_parameters\n",
    "    print('total number of trainable parameters: ',total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
